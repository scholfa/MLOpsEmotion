version: "3.9"
services:

  # ─── App: Streamlit + Prefect + DVC ───
  app:
    build:
      context: .
      dockerfile: docker/app/Dockerfile
    image: mlopsemotion-app
    env_file:
      - .env
    working_dir: /app
    volumes:
      - app-data:/app/data
      - ${HOME}/.secrets/gdrive-sa.json:/app/secret/gdrive-sa.json:ro,z
    ports:
      - "8501:8501"  # Streamlit
      - "4200:4200"  # Prefect UI/API
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_ENABLE_SYSTEM_METRICS_LOGGING=true
    depends_on:
      - mlflow
      - inference
    networks:
      - app-net

  # ─── MLflow Server ───
  mlflow:
    build:
      context: .
      dockerfile: docker/mlflow/Dockerfile
    image: mlopsemotion-mlflow
    ports:
      - "5000:5000"
      - "5001:5001"
    environment:
      - MLFLOW_TRACKING_URI=http://mlflow:5000
      - MLFLOW_HTTP_REQUEST_TIMEOUT=600
    networks:
      - app-net

  # ─── Inference: PyTorch + FastAPI ───
  inference:
    build:
      context: .
      dockerfile: docker/inference/Dockerfile
    image: mlopsemotion-inference
    env_file:
      - .env
    working_dir: /app
    ports:
      - "8000:8000"
    volumes:
      - app-data:/app/data
    command: [ "uvicorn" , "inference_api:app", "--reload", "--host", "0.0.0.0", "--port", "8000" ]
    networks:
      - app-net

volumes:
  app-data:

networks:
  app-net:
    driver: bridge
